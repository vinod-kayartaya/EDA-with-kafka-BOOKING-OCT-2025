# **Testing Kafka Producer and Consumer in Spring Boot Applications**

Testing Kafka-based applications is one of the trickier aspects of event-driven microservice development. Unlike traditional REST services, Kafka applications involve asynchronous event streams, schema validation, and distributed infrastructure — all of which require careful test design.

In this article, we’ll explore **how to write effective unit and integration tests** for a **Spring Boot Kafka Producer and Consumer** that use **Avro serialization** with the **Confluent Schema Registry**.

We’ll use a **banking domain example** for clarity.

## **1. Understanding What to Test**

Before jumping into code, it’s crucial to separate **unit** and **integration** testing goals.

| Test Type            | Purpose                                                  | Kafka Connection | Schema Registry              |
| -------------------- | -------------------------------------------------------- | ---------------- | ---------------------------- |
| **Unit Test**        | Test logic in isolation (no external services)           | Not used         | Not used                     |
| **Integration Test** | Test producer/consumer behavior with actual message flow | Embedded Kafka   | Mock or real Schema Registry |

This distinction ensures tests are fast, reliable, and maintainable.

## **2. The Example Scenario**

Our example application publishes **bank transactions** to a Kafka topic using Avro.
Each event conforms to an Avro schema registered with the Schema Registry.

**Avro schema (simplified):**

```json
{
  "namespace": "com.example.banking.avro",
  "type": "record",
  "name": "BankTransaction",
  "fields": [
    { "name": "transactionId", "type": "string" },
    { "name": "accountNumber", "type": "string" },
    { "name": "amount", "type": "double" },
    { "name": "transactionType", "type": "string" }
  ]
}
```

## **3. Setting Up Dependencies**

You’ll need the following dependencies in your `pom.xml`:

```xml
<dependencies>
    <!-- Kafka -->
    <dependency>
        <groupId>org.springframework.kafka</groupId>
        <artifactId>spring-kafka</artifactId>
    </dependency>

    <!-- Confluent Avro Serializer -->
    <dependency>
        <groupId>io.confluent</groupId>
        <artifactId>kafka-avro-serializer</artifactId>
        <version>7.6.0</version>
    </dependency>

    <!-- Testing -->
    <dependency>
        <groupId>org.springframework.kafka</groupId>
        <artifactId>spring-kafka-test</artifactId>
        <scope>test</scope>
    </dependency>
</dependencies>
```

The same setup works for both producer and consumer applications.

## **4. Unit Testing the Kafka Producer**

Unit tests focus purely on business logic. The producer’s job is to **publish a message** using `KafkaTemplate`.
You can easily mock this template using **Mockito**.

**Test objective:** Verify that the producer sends the correct message to the correct topic.

**Snippet:**

```java
KafkaTemplate<String, BankTransaction> kafkaTemplate = Mockito.mock(KafkaTemplate.class);
BankTransactionProducer producer = new BankTransactionProducer(kafkaTemplate);

BankTransaction txn = new BankTransaction("T1001", "A200", 2500.0, "debit");
producer.sendTransaction(txn);

verify(kafkaTemplate, times(1))
    .send(eq("bank-transactions"), eq("T1001"), eq(txn));
```

Here:

- No Kafka cluster is started.
- We simply assert that the `KafkaTemplate` was called as expected.
- This ensures the producer logic (e.g., topic name, key, and message type) works correctly.

## **5. Unit Testing the Kafka Consumer**

A consumer can often be tested as a **plain method call**, especially if it doesn’t depend on Kafka-specific infrastructure.

**Test objective:** Verify that the consumer processes the deserialized message correctly.

**Snippet:**

```java
BankTransactionConsumer consumer = new BankTransactionConsumer();
BankTransaction txn = new BankTransaction("T2001", "A500", 10000.0, "credit");

consumer.consume(txn);

// Add assertions or mocks for any service calls the consumer triggers
```

If your consumer interacts with a service or repository, you can mock those dependencies to verify behavior.

## **6. Integration Testing with Embedded Kafka**

Unit tests alone don’t ensure that serialization, deserialization, and schema validation work correctly.
For that, we need **integration tests**.

Spring Kafka provides an `@EmbeddedKafka` annotation that spins up a lightweight Kafka broker inside your test JVM.

We’ll combine that with a **Mock Schema Registry Client** to simulate the schema registry.

### **a. Test Configuration**

Use `@EmbeddedKafka` and `@SpringBootTest` to create a self-contained integration test:

```java
@SpringBootTest
@EmbeddedKafka(partitions = 1, topics = {"bank-transactions"})
class KafkaIntegrationTest {
    @Autowired
    private EmbeddedKafkaBroker embeddedKafka;
}
```

### **b. Creating Kafka Producer and Consumer Factories**

Inside your test, create Kafka `ProducerFactory` and `ConsumerFactory` configured for Avro serialization:

```java
Map<String, Object> producerProps = KafkaTestUtils.producerProps(embeddedKafka);
producerProps.put("key.serializer", StringSerializer.class);
producerProps.put("value.serializer", KafkaAvroSerializer.class);
producerProps.put("schema.registry.url", "mock://registry");
```

For consuming:

```java
Map<String, Object> consumerProps = KafkaTestUtils.consumerProps("testGroup", "true", embeddedKafka);
consumerProps.put("key.deserializer", StringDeserializer.class);
consumerProps.put("value.deserializer", KafkaAvroDeserializer.class);
consumerProps.put("specific.avro.reader", true);
consumerProps.put("schema.registry.url", "mock://registry");
```

These settings tell Kafka to use a **mock schema registry** instead of a real one.

### **c. Sending and Receiving a Message**

With factories ready, you can create a Kafka template, send a message, and verify it was received:

```java
BankTransaction txn = new BankTransaction("T3001", "A900", 7500.0, "debit");
kafkaTemplate.send("bank-transactions", txn.getTransactionId(), txn);

ConsumerRecord<String, BankTransaction> record = records.poll(10, TimeUnit.SECONDS);
assertThat(record.value().getTransactionId()).isEqualTo("T3001");
```

This round-trip confirms that:

- Avro serialization and deserialization are functioning.
- Schema compatibility is enforced.
- The consumer receives valid Avro objects.

## **7. Integration Testing with Spring Context**

If your producer and consumer are part of a larger Spring Boot application, you can write simpler context-level tests:

```java
@SpringBootTest
@EmbeddedKafka(partitions = 1, topics = {"bank-transactions"})
class BankAppIntegrationTest {

    @Autowired
    private BankTransactionProducer producer;

    @Autowired
    private BankTransactionConsumer consumer;

    @Test
    void contextLoads() {
        assertNotNull(producer);
        assertNotNull(consumer);
    }
}
```

You can extend this further to trigger actual producer calls and verify consumer output.

## **8. Schema Validation in Tests**

Even with a mock schema registry, Avro serializers still perform **schema-based validation**.
If your message doesn’t conform to the registered schema (e.g., missing fields, wrong type), serialization will fail — just like in production.

This helps you catch schema mismatches early.

## **9. Best Practices Summary**

| Area                  | Recommendation                                                  |
| --------------------- | --------------------------------------------------------------- |
| **Producer Tests**    | Mock `KafkaTemplate` and verify topic, key, and message         |
| **Consumer Tests**    | Call consumer methods directly and assert side effects          |
| **Integration Tests** | Use `@EmbeddedKafka` and a mock schema registry                 |
| **Schema Evolution**  | Use backward/forward compatibility checks in schema registry    |
| **Performance**       | Keep integration tests minimal — focus on critical paths        |
| **Realistic Tests**   | For CI/CD, consider Testcontainers with Kafka + Schema Registry |

## **10. Conclusion**

Testing Kafka applications in Spring Boot is a layered process.
Start simple — isolate your producer and consumer logic with unit tests, then graduate to integration tests that validate end-to-end behavior.

By combining **Spring Kafka’s embedded testing tools** with **Avro’s schema validation**, you ensure that:

- Your applications produce and consume messages safely,
- Schema changes don’t break your pipelines,
- And your code remains testable and maintainable.
