# Capstone Project Case Study

## Real-Time Payment Processing System using Apache Kafka

### 1. Project Overview

In the modern digital economy, financial institutions and payment service providers are expected to process large volumes of transactions in real time, ensuring high availability, reliability, and compliance with fraud and anti-money laundering (AML) regulations.

This capstone project aims to design and implement a **Real-Time Payment Processing System** leveraging **Apache Kafka** as the central event-streaming backbone. The system will simulate multiple banking applications producing payment transactions that are processed through fraud/AML checks, settled across accounts, and trigger customer notifications — all in real time.

Participants will implement a distributed, event-driven architecture demonstrating **Kafka Streams, schema validation, observability, and end-to-end reliability** through testing and dead-letter mechanisms.

### 2. Business Problem Statement

Banks and financial organizations handle thousands of payment transactions per second across multiple digital channels (mobile, web, ATM, POS). These transactions must be processed with:

- **High throughput and low latency**
- **Guaranteed message delivery**
- **Real-time fraud detection and compliance checks**
- **Accurate settlement and reconciliation**
- **Timely customer notifications**

Traditional batch systems cannot meet these requirements due to their inherent latency and scalability limitations. The challenge is to design a **streaming-based real-time pipeline** that ensures **data integrity, resilience, and observability** while maintaining modularity across the microservices.

### 3. Objective

To build a **Kafka-based real-time payment processing system** that simulates a realistic end-to-end payment lifecycle, demonstrating the use of:

- **Kafka Producers** and **Consumers**
- **Kafka Streams / ksqlDB** for in-stream processing
- **Schema Registry** and **DLT (Dead Letter Topics)** for data validation
- **Unit and Integration Testing** using Embedded Kafka
- **Monitoring and Observability** through Prometheus and Grafana dashboards

### 4. Functional Flow

#### High-Level Flow:

1. **Producers (Banking Applications)**
   Multiple producer services simulate payment events (e.g., UPI, NEFT, IMPS).
   Each event contains transaction ID, account details, type, amount, and timestamp.

2. **Kafka Ingestion Layer**
   Payments are published to a `payments.raw` Kafka topic.
   Schema validation occurs using Confluent Schema Registry or Avro schema.

3. **Fraud & AML Check**
   A stream-processing application consumes validated payments from `payments.raw`, performs fraud/AML checks, and publishes results to:

   - `payments.validated` (passed checks)
   - `payments.fraudulent` (failed checks)

4. **Settlement Service**
   Consumes from `payments.validated`, updates ledger/settlement data, and produces settlement confirmation events to `payments.settled`.

5. **Notification Service**
   Consumes from `payments.settled` and triggers simulated SMS/Email notifications to customers.

6. **DLT (Dead Letter Topic)**
   Any message failing deserialization or schema validation is routed to `payments.dlt` with the reason logged.

7. **Observability & Metrics**
   Expose application and topic metrics using **Micrometer + Prometheus**, visualized in **Grafana**.

### 5. System Architecture

**Key Components:**

- **Producers:** Simulate multiple bank applications generating transactions.
- **Kafka Cluster:** Central event streaming backbone with multiple topics (`raw`, `validated`, `fraudulent`, `settled`, `dlt`).
- **Fraud/AML Microservice:** Stream processor for compliance checks.
- **Settlement Microservice:** Handles fund settlement and confirmation.
- **Notification Microservice:** Sends out notifications for completed transactions.
- **Schema Registry:** Validates message structure using Avro or JSON Schema.
- **Prometheus & Grafana:** For system metrics and dashboards.
- **Test Framework:** Unit and integration testing with Embedded Kafka and JUnit.

### 6. Technical Requirements

| Component         | Technology / Library                 |
| ----------------- | ------------------------------------ |
| Event Broker      | Apache Kafka                         |
| Stream Processing | Kafka Streams or Spring Cloud Stream |
| Serialization     | JSON or Avro with Schema Registry    |
| DLT Handling      | Kafka Dead Letter Topics             |
| Testing           | JUnit 5, Embedded Kafka              |
| Monitoring        | Micrometer, Prometheus, Grafana      |
| Logging           | Logback / SLF4J                      |
| Build Tool        | Maven or Gradle                      |
| Language          | Java (Spring Boot)                   |

### 7. Scope of Work

#### **1. Kafka Setup**

- Configure Kafka cluster with required topics:

  - `payments.raw`
  - `payments.validated`
  - `payments.fraudulent`
  - `payments.settled`
  - `payments.dlt`

- Set appropriate partitioning and replication factor.

#### **2. Producer Applications**

- Simulate multiple banking producers (2–3 applications) publishing transaction events to `payments.raw`.
- Use unique producer IDs for each source.

#### **3. Schema Validation and DLT**

- Implement schema validation using Schema Registry.
- Handle invalid or unparseable messages by redirecting them to `payments.dlt`.
- Include metadata: `error_reason`, `timestamp`, `source_app`.

#### **4. Fraud/AML Service**

- Consume messages from `payments.raw`.
- Apply mock fraud rules (e.g., “amount > 100000” or “same account transfers within 10 seconds”).
- Produce validated/fraudulent messages to their respective topics.

#### **5. Settlement Service**

- Consume validated transactions.
- Simulate balance updates and settlement confirmation.
- Publish results to `payments.settled`.

#### **6. Notification Service**

- Consume settlement messages.
- Simulate notification generation (logging to console or storing in a notification topic).

#### **7. Observability and Monitoring**

- Expose application metrics (processed count, failed count, latency, etc.) using Micrometer and Prometheus.
- Create Grafana dashboards for:

  - Throughput per topic
  - Failed message counts (DLT)
  - Processing latency
  - Service uptime

#### **8. Testing**

- Implement:

  - **Unit tests** for message validation and fraud logic.
  - **Integration tests** using Embedded Kafka.

- Ensure no production code contains test-specific logic (e.g., CountDownLatch inside business logic).

#### **9. Documentation**

- Architecture diagram
- Kafka topic schema definitions
- Service-to-service data flow
- Test strategy
- Dashboard snapshots

### 8. Deliverables

1. **Source Code Repository** (GitHub or GitLab)
   Organized by microservice/module with a `README.md` for setup instructions.

2. **Architecture Documentation**
   Explaining event flow, topic definitions, schema evolution strategy, and error handling design.

3. **Test Reports**
   Coverage reports and logs from unit/integration tests.

4. **Monitoring Dashboard**
   Prometheus/Grafana configuration files and screenshots.

5. **Demo Video (Optional)**
   A short walkthrough of the system in action.

### 9. Evaluation Criteria

| Evaluation Aspect                        | Weightage |
| ---------------------------------------- | --------- |
| Functional Correctness & End-to-End Flow | 25%       |
| Kafka Architecture & Event Design        | 20%       |
| Schema Validation & DLT Handling         | 15%       |
| Test Coverage & Code Quality             | 15%       |
| Observability (Metrics, Dashboards)      | 15%       |
| Documentation & Presentation             | 10%       |

### 10. Expected Outcome

By completing this project, participants will:

- Gain hands-on experience in building a **production-grade Kafka-based streaming system**.
- Understand **schema evolution, data quality enforcement, and error handling patterns**.
- Apply **observability and testing best practices** in real-time data pipelines.
- Develop a reusable **microservices blueprint** for event-driven architectures in FinTech systems.
