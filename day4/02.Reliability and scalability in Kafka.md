# Reliability and Scaling in Apache Kafka — A Practical Guide

Apache Kafka is designed to handle massive data streams with high throughput and fault tolerance. But out of the box, Kafka’s reliability and scalability depend heavily on how you configure it.

In this guide, we’ll explore how Kafka ensures reliability, how it scales, and what configurations you can use to achieve enterprise-grade performance.

## Part 1: Understanding Reliability in Kafka

Kafka provides reliability through replication, acknowledgements, and durability guarantees.

Imagine a topic called `transactions` spread across 5 brokers in a cluster:

```
Brokers: B1, B2, B3, B4, B5
Topic: transactions
Partitions: 3
Replication Factor: 3
```

Each partition (say `P0`) will have:

- 1 leader broker (handles reads/writes)
- 2 followers (replicas for redundancy)

When the leader fails, one of the followers automatically becomes the new leader — ensuring zero data loss or downtime (if configured correctly).

### 1. Replication — The Foundation of Reliability

Replication ensures that each partition has multiple copies distributed across brokers.

#### Example Configuration

In `server.properties` of each broker:

```properties
# Default replication factor for topics
default.replication.factor=3

# Minimum number of in-sync replicas required for producer acknowledgements
min.insync.replicas=2
```

This means each topic will have three copies (one leader + two followers), and Kafka requires at least two brokers in sync before acknowledging a write.

#### Producer-side Reliability Config

On the producer:

```java
props.put("acks", "all");
props.put("retries", Integer.MAX_VALUE);
props.put("max.in.flight.requests.per.connection", "1");
props.put("enable.idempotence", "true");
```

Setting `acks=all` and `min.insync.replicas >= 2` provides strong durability.
If `acks=1`, messages can be lost if the leader crashes before followers sync.

### 2. In-Sync Replicas (ISR)

The ISR list represents replicas fully caught up with the leader.
A replica not in ISR cannot serve as a leader candidate.

You can view ISR using the Kafka CLI:

```bash
kafka-topics.sh --describe --topic transactions --bootstrap-server localhost:9092
```

Example output:

```
Topic: transactions   Partition: 0   Leader: 2   Replicas: 2,3,5   Isr: 2,3
```

Here, broker 5 is out of sync — it will not be elected leader until it catches up.

### 3. Durability and Data Retention

Kafka persists all messages to disk. To improve durability, enable appropriate log flush and fsync configurations:

```properties
# Force flush to disk after every message (not recommended for high throughput)
flush.messages=1

# Force flush every 5 seconds
flush.ms=5000

# Retain messages for 7 days
log.retention.hours=168
```

Kafka relies on the OS page cache for performance, so avoid over-flushing.
Instead, rely on replication for durability.

## Part 2: Scaling Kafka — Horizontally and Vertically

Kafka achieves scalability primarily through partitioning topics and distributing load across brokers.

### 1. Scaling Producers

Producers can scale horizontally — each producer can write to any partition based on key or round-robin assignment.

Example:

```java
producer.send(new ProducerRecord<>("transactions", "account-123", "debit=500"));
```

If key-based partitioning is used (here, `account-123`), all transactions for the same key go to the same partition, preserving order.

### 2. Scaling Brokers

To scale out Kafka:

- Add more brokers
- Rebalance partitions across them

Start a new broker:

```bash
kafka-server-start.sh config/server-4.properties
```

Rebalance partitions using:

```bash
kafka-reassign-partitions.sh --execute --reassignment-json-file reassignment.json
```

Example `reassignment.json`:

```json
{
  "version": 1,
  "partitions": [
    { "topic": "transactions", "partition": 0, "replicas": [2, 3, 4] },
    { "topic": "transactions", "partition": 1, "replicas": [3, 4, 5] }
  ]
}
```

### 3. Scaling Consumers

Kafka’s consumer group model ensures that:

- Each partition is processed by only one consumer in a group.
- Adding more consumers increases parallelism and throughput.

Example configuration:

```java
props.put("group.id", "txn-processor-group");
props.put("enable.auto.commit", "false");
props.put("max.poll.records", "500");
```

Adding a new consumer instance to the same group will automatically rebalance and increase processing capacity.

### 4. Scaling Topics and Partitions

You can increase partitions dynamically:

```bash
kafka-topics.sh --alter --topic transactions --partitions 6 --bootstrap-server localhost:9092
```

**Note:** Increasing partitions breaks ordering guarantees for keys unless partition keys are used consistently.

## Part 3: Example — Reliable and Scalable Kafka Setup

Here’s a sample production-grade setup for a 5-broker Kafka cluster.

### server.properties (common configs)

```properties
broker.id=1
log.dirs=/var/lib/kafka/data
num.partitions=6
default.replication.factor=3
min.insync.replicas=2
auto.create.topics.enable=false

# Networking and replication
listeners=PLAINTEXT://:9092
advertised.listeners=PLAINTEXT://broker1:9092
replica.lag.time.max.ms=10000

# Log retention
log.retention.hours=168
log.segment.bytes=1073741824
log.cleanup.policy=delete
```

### producer.properties

```properties
acks=all
retries=2147483647
max.in.flight.requests.per.connection=1
enable.idempotence=true
compression.type=snappy
```

### consumer.properties

```properties
group.id=txn-processor-group
enable.auto.commit=false
auto.offset.reset=latest
fetch.min.bytes=50000
max.poll.records=1000
```

## Part 4: Testing Reliability and Scaling

You can test reliability by simulating broker failures:

```bash
docker stop kafka-2
```

Then observe automatic leader election:

```bash
kafka-topics.sh --describe --topic transactions --bootstrap-server localhost:9092
```

To measure scaling and throughput:

```bash
kafka-producer-perf-test.sh \
  --topic transactions \
  --num-records 1000000 \
  --record-size 100 \
  --throughput -1 \
  --producer-props acks=all bootstrap.servers=localhost:9092
```

## Summary

| Aspect               | Configuration                  | Purpose                     |
| -------------------- | ------------------------------ | --------------------------- |
| Replication          | `default.replication.factor=3` | Ensures redundancy          |
| Acknowledgements     | `acks=all`                     | Guarantees durability       |
| In-Sync Replicas     | `min.insync.replicas=2`        | Avoids data loss            |
| Partitioning         | `num.partitions`               | Controls scalability        |
| Consumer Groups      | `group.id`                     | Enables parallel processing |
| Producer Idempotence | `enable.idempotence=true`      | Avoids duplicate records    |

### Key Takeaways

- Reliability in Kafka = Replication + ISR + Proper Acks
- Scaling in Kafka = More Partitions + More Brokers + Consumer Groups
- Always monitor ISR, under-replicated partitions, and consumer lag.
- Use metrics from Prometheus and Grafana for observability.
